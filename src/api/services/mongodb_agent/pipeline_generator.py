"""
Pipeline generation utilities powered by the OpenAI Responses API.
"""

from __future__ import annotations

import json
import logging
from typing import Any, Dict, List, Optional, Tuple

from openai import OpenAI

from .prompts import build_static_prompt_prefix
from .schema import MongoDBSchemaContext


class PipelineGenerationResult(Tuple[Optional[List[Dict[str, Any]]], Optional[str], Optional[Any], Optional[str]]):
    """Typed alias for pipeline generation return value."""


class PipelineGenerator:
    """Generate MongoDB aggregation pipelines using GPT models."""

    def __init__(self, client: OpenAI, model_name: str) -> None:
        self.client = client
        self.model_name = model_name
        self.logger = logging.getLogger(__name__)
        self.static_prompt_prefix = build_static_prompt_prefix(MongoDBSchemaContext.get_schema())

    def set_model_name(self, model_name: str) -> None:
        self.model_name = model_name

    def _supports_reasoning_effort(self) -> bool:
        return isinstance(self.model_name, str) and self.model_name.lower().startswith("gpt-5")

    def _normalize_verbosity(self, desired: str) -> str:
        return desired if self._supports_reasoning_effort() else "medium"

    def generate(
        self,
        *,
        question: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        previous_error: Optional[str] = None,
        previous_response_id: Optional[str] = None,
        reasoning_effort: Optional[str] = None,
    ) -> Tuple[Optional[List[Dict[str, Any]]], Optional[str], Optional[Any], Optional[str]]:
        """
        Generate a MongoDB pipeline for the given question.

        Returns:
            pipeline, response_id, raw_response, error_message
        """
        conversation_context = self._build_conversation_context(conversation_history, previous_error)
        dynamic_suffix = f"""{conversation_context}

QUESTION: {question}

Return ONLY the JSON array, nothing else:"""

        input_prompt = self.static_prompt_prefix + "\n\n" + dynamic_suffix

        request_params: Dict[str, Any] = {
            "model": self.model_name,
            "input": input_prompt,
            "text": {"verbosity": self._normalize_verbosity("low")},
            "max_output_tokens": 4000,
        }

        if self._supports_reasoning_effort():
            request_params["reasoning"] = {"effort": reasoning_effort or "medium"}

        if previous_response_id:
            request_params["previous_response_id"] = previous_response_id

        response = self.client.responses.create(**request_params)
        new_response_id = response.id

        output_text = self._extract_output_text(response)
        if not output_text:
            return None, new_response_id, response, "No output generated by model"

        try:
            json_match = self._extract_json_array(output_text)
        except ValueError as exc:
            return None, new_response_id, response, str(exc)

        try:
            pipeline = json.loads(json_match)
            if not isinstance(pipeline, list):
                raise ValueError("Model response is not a JSON array")
            self.logger.info("Successfully extracted pipeline with %d stages", len(pipeline))
            return pipeline, new_response_id, response, None
        except (json.JSONDecodeError, ValueError) as exc:
            self.logger.error("Failed to parse JSON pipeline: %s", exc)
            return None, new_response_id, response, f"JSON parsing error: {exc}"

    def _build_conversation_context(
        self,
        history: Optional[List[Dict[str, Any]]],
        previous_error: Optional[str],
    ) -> str:
        if not history and not previous_error:
            return ""

        context_parts: List[str] = []
        if history:
            context_parts.append("\nCONVERSATION HISTORY:")
            for index, exchange in enumerate(history[-3:], 1):
                user_q = exchange.get("question") or exchange.get("query", "N/A")
                response = exchange.get("response", {})
                result_summary = self._summarize_response(response)
                context_parts.append(f"{index}. Previous Question: {user_q}")
                context_parts.append(f"   {result_summary}")

        if previous_error:
            context_parts.append(f"\nPREVIOUS ATTEMPT FEEDBACK:\n{previous_error}")

        return "\n".join(context_parts)

    def _summarize_response(self, response: Any) -> str:
        if isinstance(response, dict):
            results = response.get("results", [])
            if results:
                preview = results[0]
                suffix = f" (and {len(results) - 1} more)" if len(results) > 1 else ""
                return f"Result: {preview}{suffix}"
            return "Result: No results"
        return f"Result: {str(response)[:100]}"

    def _extract_output_text(self, response: Any) -> str:
        if hasattr(response, "output_text") and response.output_text:
            text = response.output_text
            self.logger.info("Got output_text: %s", text[:200])
            return text

        output_text = ""
        items = getattr(response, "output", None)
        if items:
            for item in items:
                if hasattr(item, "content") and item.content and isinstance(item.content, str):
                    output_text += item.content
                elif hasattr(item, "text"):
                    output_text += item.text
        if output_text:
            self.logger.info("Extracted from output items: %s", output_text[:200])
        return output_text

    def _extract_json_array(self, source_text: str) -> str:
        start_idx = source_text.find("[")
        end_idx = source_text.rfind("]")
        if start_idx == -1 or end_idx == -1 or end_idx <= start_idx:
            raise ValueError("Failed to parse JSON array from model output")
        return source_text[start_idx : end_idx + 1]
